{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cf421d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import process_tweet\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2ec5cd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\pc1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pc1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the dataset from nltk\n",
    "nltk.download('twitter_samples')\n",
    "\n",
    "# stop words are common words that we don't want to include in our features\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e9a804e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape = 6988\n",
      "test_x.shape = 1000\n",
      "train_y.shape = 6988\n",
      "test_y.shape = 1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...</td>\n",
       "      <td>celebrity</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...</td>\n",
       "      <td>info_news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...</td>\n",
       "      <td>info_news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...</td>\n",
       "      <td>celebrity</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...</td>\n",
       "      <td>personal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   category  stance\n",
       "0  بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...  celebrity       1\n",
       "1  وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...  info_news       1\n",
       "2  قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...  info_news       1\n",
       "3  #تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...  celebrity       1\n",
       "4  وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...   personal       0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read train/dev sets\n",
    "df = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('dev.csv')\n",
    "\n",
    "train_x = df['text'].tolist() \n",
    "test_x =df_test['text'].tolist()\n",
    "\n",
    "train_y1 = df['stance'].tolist()\n",
    "test_y1 = df_test['stance'].tolist()\n",
    "\n",
    "train_y2 = df['category'].tolist()\n",
    "test_y2 = df_test['category'].tolist()\n",
    "\n",
    "# Print the shape train and test sets\n",
    "print(\"train_x.shape = \" + str(len(train_x)))\n",
    "print(\"test_x.shape = \" + str(len(test_x)))\n",
    "print(\"train_y.shape = \" + str(len(train_y1)))\n",
    "print(\"test_y.shape = \" + str(len(test_y1)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "87c5c494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process tweets\n",
    "def process_tweets(x):\n",
    "    X = []\n",
    "    for tweet in x:\n",
    "        X.append(process_tweet(tweet))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6a3232c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get unique tokens and thier frequancies in all tweets\n",
    "def get_tokens(X):\n",
    "    tokens = {}\n",
    "    for tweet in X:\n",
    "        for token in tweet:\n",
    "            if token not in tokens.keys():\n",
    "                tokens[token] = 0\n",
    "\n",
    "    #copy = tokens.copy()\n",
    "    #for key,val in copy.items():\n",
    "    #    if val<4: tokens.pop(key)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3fa14c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الابرة و لا السيرنجة و لا الدواء و لابس بولو صيفي في عز الشتاء و يقول ان إحدى مزايا عمر ال 65 عامًا هي انه مؤهل للحصول على اللقاح ... يعنى ما كان يحتاج اللقاح لو كان عمره اصغر من 65 🤔 https://t.co/QQKFFUNwBn\n",
      "['بيل', 'غيتس', 'لقى', 'لقح', 'كوفيد', '19', 'صور', 'ابر', 'رنج', 'دوء', 'لبس', 'ولو', 'صيف', 'عز', 'شتء', 'يقل', 'ان', 'مزا', 'عمر', 'ال', '65', 'عما', 'انه', 'ؤهل', 'حصل', 'لقح', 'عنى', 'حاج', 'لقح', 'عمر', 'صغر', '65', '🤔']\n"
     ]
    }
   ],
   "source": [
    "train_input = process_tweets(train_x)\n",
    "print(train_x[0])\n",
    "print(train_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8e548fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12194"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Unique_tokens = get_tokens(train_input)\n",
    "j = 0\n",
    "for i in Unique_tokens.keys():\n",
    "  Unique_tokens[i] = j\n",
    "  j+=1\n",
    "num_tokens = len(Unique_tokens)\n",
    "for i in train_input:\n",
    "  for index,token in enumerate(i):\n",
    "    i[index] = Unique_tokens[token]\n",
    "len(Unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "409e7b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, x, y, pad):\n",
    "    \"\"\"\n",
    "    This is the constructor of the RNNDataset\n",
    "    Inputs:\n",
    "    - x: a list of lists where each list contains the ids of the tokens\n",
    "    - y: a list of lists where each list contains the label of each token in the sentence\n",
    "    - pad: the id of the <PAD> token (to be used for padding all sentences and labels to have the same length)\n",
    "    \"\"\"\n",
    "    ##################### create two tensors one for x and the other for labels ###############################\n",
    "    max_len = max([len(i) for i in x])    \n",
    "    pad_size = [max_len - len(i) for i in x]\n",
    "    x = [ ( j + [pad] * pad_size[i])[:max_len] for i,j in enumerate(x)]  \n",
    "    self.inputs = torch.tensor(x)\n",
    "    self.lables = torch.tensor(y)\n",
    "    inds = [self.lables == -1]\n",
    "    self.lables[inds] = 2\n",
    "    #################################################################################################################\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "    This function should return the length of the dataset (the number of sentences)\n",
    "    \"\"\"\n",
    "    ###################### return the length of the dataset #############################\n",
    "    return len( self.inputs )\n",
    "    ###########################################################################################\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\"\n",
    "    This function returns a subset of the whole dataset\n",
    "    \"\"\"\n",
    "    ###################### return a tuple of x and y ###################################\n",
    "    return ( self.inputs[idx],self.lables[idx])\n",
    "    ##########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8f483491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512]) torch.Size([512, 113]) torch.Size([512]) torch.Size([512, 113])\n",
      "tensor([    0,     1,     2,     3,     4,     5,     6,     7,     8,     9,\n",
      "           10,    11,    12,    13,    14,    15,    16,    17,    18,    19,\n",
      "           20,    21,    22,    23,    24,     3,    25,    26,     3,    18,\n",
      "           27,    20,    28, 12194, 12194, 12194, 12194, 12194, 12194, 12194,\n",
      "        12194, 12194, 12194, 12194, 12194, 12194, 12194, 12194, 12194, 12194,\n",
      "        12194, 12194, 12194, 12194, 12194, 12194, 12194, 12194, 12194, 12194,\n",
      "        12194, 12194, 12194, 12194, 12194, 12194, 12194, 12194, 12194, 12194,\n",
      "        12194, 12194, 12194, 12194, 12194, 12194, 12194, 12194, 12194, 12194,\n",
      "        12194, 12194, 12194, 12194, 12194, 12194, 12194, 12194, 12194, 12194,\n",
      "        12194, 12194, 12194, 12194, 12194, 12194, 12194, 12194, 12194, 12194,\n",
      "        12194, 12194, 12194, 12194, 12194, 12194, 12194, 12194, 12194, 12194,\n",
      "        12194, 12194, 12194]) \n",
      " tensor(1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6988"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_dataset = RNNDataset(train_input, train_y1, len(Unique_tokens))\n",
    "dummy_dataloader = torch.utils.data.DataLoader(dummy_dataset, batch_size=512)\n",
    "dg = iter(dummy_dataloader)\n",
    "X1, Y1 = next(dg)\n",
    "X2, Y2 = next(dg)\n",
    "print(Y1.shape, X1.shape, Y2.shape, X2.shape)\n",
    "print(X1[0][:], \"\\n\", Y1[0])\n",
    "len(dummy_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2a89472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "  def __init__(self, vocab_size=13000, embedding_dim=50, hidden_size=50, n_classes=3):\n",
    "    \n",
    "    \"\"\"\n",
    "    The constructor of our RNN model\n",
    "    Inputs:\n",
    "    - vacab_size: the number of unique words\n",
    "    - embedding_dim: the embedding dimension\n",
    "    - n_classes: the number of final classes \n",
    "    \"\"\"\n",
    "    super(RNN, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_classes = n_classes\n",
    "    ####################### Create the layers of your model #######################################\n",
    "    # (1) Create the embedding layer\n",
    "    self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "\n",
    "    # (2) Create an RNN layer with hidden size = hidden_size and batch_first = True\n",
    "    self.rnn = nn.RNN(input_size = embedding_dim,hidden_size = hidden_size,batch_first = True)\n",
    "\n",
    "    # (3) Create a linear layer with number of neorons = n_classes\n",
    "    self.linear = nn.Linear(hidden_size,n_classes)\n",
    "    #####################################################################################################\n",
    "\n",
    "  def forward(self, sentences):\n",
    "\n",
    "    final_output = None\n",
    "    ######################### implement the forward pass ####################################\n",
    "    h_0 = torch.zeros(1,sentences.size(0), self.hidden_size)\n",
    "    #print(sentences.shape)\n",
    "    embedding_out = self.embedding(sentences)\n",
    "    \n",
    "    rnn_out,_ = self.rnn(embedding_out, h_0)\n",
    "    rnn_out = rnn_out[:, -1,:]\n",
    "    \n",
    "    final_output = self.linear(rnn_out)\n",
    "    ###############################################################################################\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ec0f032e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(13000, 50)\n",
       "  (rnn): RNN(50, 50, batch_first=True)\n",
       "  (linear): Linear(in_features=50, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNN()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "711d1137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, batch_size=512, epochs=5, learning_rate=0.01):\n",
    "  \"\"\"\n",
    "  This function implements the training logic\n",
    "  Inputs:\n",
    "  - model: the model ot be trained\n",
    "  - train_dataset: the training set of type RNNDataset\n",
    "  - batch_size: integer represents the number of examples per step\n",
    "  - epochs: integer represents the total number of epochs (full training pass)\n",
    "  - learning_rate: the learning rate to be used by the optimizer\n",
    "  \"\"\"\n",
    "    \n",
    "  # (1) create the dataloader of the training set (make the shuffle=True)\n",
    "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
    "\n",
    "  # (2) make the criterion cross entropy loss\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  # (3) create the optimizer (Adam)\n",
    "  optimizer = torch.optim.Adam(model.parameters(),learning_rate)\n",
    "\n",
    "  # GPU configuration\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  if use_cuda:\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "  for epoch_num in range(epochs):\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "\n",
    "    for train_input, train_label in tqdm(train_dataloader):\n",
    "      #try:\n",
    "      #  train_input = train_input.reshape(-1,104,512)\n",
    "      #except:\n",
    "      #  continue\n",
    "     \n",
    "      # (4) move the train input to the device\n",
    "      train_label = train_label.to(device)\n",
    "\n",
    "      # (5) move the train label to the device\n",
    "      train_input = train_input.to(device)\n",
    "\n",
    "      # (6) do the forward pass\n",
    "      output = model(train_input)\n",
    "      \n",
    "      # (7) loss calculation \n",
    "      batch_loss = criterion(output, train_label)\n",
    "      \n",
    "      # (8) append the batch loss to the total_loss_train\n",
    "      total_loss_train += batch_loss\n",
    "      \n",
    "      # (9) calculate the batch accuracy\n",
    "\n",
    "      _,pred = torch.max(output,1)\n",
    "      acc = (pred == train_label).sum().item()\n",
    "      total_acc_train += acc\n",
    "\n",
    "      # (10) zero your gradients\n",
    "      model.zero_grad()\n",
    "\n",
    "      # (11) do the backward pass\n",
    "      batch_loss.backward()\n",
    "\n",
    "      # (12) update the weights with your optimizer\n",
    "      optimizer.step()\n",
    "      \n",
    "    # epoch loss\n",
    "    epoch_loss = total_loss_train / len(train_dataset)\n",
    "\n",
    "    # (13) calculate the accuracy\n",
    "    epoch_acc = 100 * total_acc_train / len(train_dataset)\n",
    "\n",
    "    print(\n",
    "        f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n",
    "        | Train Accuracy: {epoch_acc}\\n')\n",
    "\n",
    "  ##############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "43d43d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "indecies = {'info_news':0, 'celebrity':1, 'plan': 2, 'requests': 3, 'rumors': 4, 'advice': 5, 'restrictions': 6, 'personal': 7,'unrelated': 8,'others': 9}\n",
    "train_y2_indecies = [indecies[k] for k in train_y2]\n",
    "test_y2_indecies = [indecies[k] for k in test_y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0c330ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RNNDataset(train_input, train_y1, len(Unique_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "237ef962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:08<00:00,  1.69it/s]\n",
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss: 0.0013335070107132196         | Train Accuracy: 79.25014310246137\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.10it/s]\n",
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss: 0.0012880577705800533         | Train Accuracy: 79.25014310246137\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:07<00:00,  1.97it/s]\n",
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss: 0.0012830737978219986         | Train Accuracy: 79.25014310246137\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:07<00:00,  1.93it/s]\n",
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss: 0.0012778631644323468         | Train Accuracy: 79.25014310246137\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:08<00:00,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss: 0.0012811814667657018         | Train Accuracy: 79.25014310246137\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = RNN(n_classes=3)\n",
    "model\n",
    "train(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e3dc57bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = process_tweets(test_x)\n",
    "Unique_tokens = get_tokens(test_input)\n",
    "j = 0\n",
    "for i in Unique_tokens.keys():\n",
    "  Unique_tokens[i] = j\n",
    "  j+=1\n",
    "num_tokens2 = len(Unique_tokens)\n",
    "for i in test_input:\n",
    "  for index,token in enumerate(i):\n",
    "    i[index] = Unique_tokens[token]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "99ad8b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = RNNDataset(train_input, train_y2_indecies, num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ab968b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:07<00:00,  1.76it/s]\n",
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss: 0.0032378770411014557         | Train Accuracy: 49.06983400114482\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:08<00:00,  1.62it/s]\n",
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss: 0.0030593248084187508         | Train Accuracy: 51.745850028620495\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:10<00:00,  1.35it/s]\n",
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss: 0.0030576493591070175         | Train Accuracy: 51.745850028620495\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:13<00:00,  1.03it/s]\n",
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss: 0.003054589033126831         | Train Accuracy: 51.745850028620495\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:12<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss: 0.0030522779561579227         | Train Accuracy: 51.745850028620495\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model2 = RNN(n_classes=10)\n",
    "model2\n",
    "train(model2, dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e300d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataset, batch_size=512):\n",
    "  \"\"\"\n",
    "  This function takes a RNN model and evaluates its performance (accuracy) on a test data\n",
    "  Inputs:\n",
    "  - model: a RNN model\n",
    "  - test_dataset: dataset of type RNNDataset\n",
    "  \"\"\"\n",
    "  \n",
    "  # (1) create the test data loader\n",
    "  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "  # GPU Configuration\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "  total_acc_test = 0\n",
    "  \n",
    "  # (2) disable gradients\n",
    "  with torch.no_grad():\n",
    "\n",
    "    for test_input, test_label in tqdm(test_dataloader):\n",
    "      # (3) move the test input to the device\n",
    "      test_label = test_label.to(device)\n",
    "\n",
    "      # (4) move the test label to the device\n",
    "      test_input = test_input.to(device)\n",
    "\n",
    "      # (5) do the forward pass\n",
    "      output = model(test_input)\n",
    "\n",
    "      # accuracy calculation\n",
    "      _,pred = torch.max(output,1)\n",
    "      acc = (pred == test_label).sum().item()\n",
    "      total_acc_test += acc\n",
    "    \n",
    "    # (6) calculate the over all accuracy\n",
    "    total_acc_test /= len(test_dataset)\n",
    "  ##################################################################################################\n",
    "\n",
    "  \n",
    "  print(f'\\nTest Accuracy: {total_acc_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6f1c009a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 18.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = RNNDataset(test_input, test_y1, num_tokens)\n",
    "evaluate(model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cf3d4739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 26.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset2 = RNNDataset(test_input, test_y2_indecies, num_tokens2)\n",
    "evaluate(model2, test_dataset2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b12a6ddb",
   "metadata": {},
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "79bff5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf(documents,tokens):\n",
    "    documents_terms = np.zeros((len(documents),len(tokens)))\n",
    "    dft = np.zeros(len(tokens))\n",
    "    for j,document in enumerate(documents):\n",
    "        for i,token in enumerate(tokens.keys()):\n",
    "            if token in document:\n",
    "                dft[i] += 1\n",
    "            documents_terms[j][i] = np.log10(documents[j].count(token)+1)\n",
    "    idf = np.zeros(len(tokens))\n",
    "    for i,j in enumerate(dft):\n",
    "        if j == 0: idf[i] = 0\n",
    "        else: idf[i] = np.log10(len(documents)/j)\n",
    "    tf_idf = documents_terms.copy()\n",
    "    for i,document in enumerate(tf_idf):\n",
    "        tf_idf[i] = document*idf\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5254c43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12194\n",
      "(6988, 12194)\n"
     ]
    }
   ],
   "source": [
    "X = process_tweets(train_x)\n",
    "tokens = get_tokens(X)\n",
    "print(len(tokens))\n",
    "tf_idf = get_tf_idf(X,tokens)\n",
    "print(tf_idf.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3368cd25",
   "metadata": {},
   "source": [
    "logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "595ad589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lr(X, Y):\n",
    "    \n",
    "    lr = LogisticRegression(max_iter=10000)\n",
    "    \n",
    "    ################################# train the lr model #####################\n",
    "    lr.fit(X,Y)\n",
    "    ###############################################################################################################\n",
    "    \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "801cc895",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = train_lr(tf_idf,train_y1)\n",
    "lr2 = train_lr(tf_idf,train_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9c88a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(clf, X):\n",
    "   \n",
    "    Y_pred = None\n",
    "    ######################### predict labels ############################\n",
    "    Y_pred = clf.predict(X)\n",
    "    #######################################################################################################################\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4ebf3649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "1000\n",
      "(1000, 12194)\n",
      "(1000,)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "test_x = np.array(test_x)\n",
    "print(test_x.shape)\n",
    "processed_tweets = process_tweets(test_x)\n",
    "print(len(processed_tweets))\n",
    "X_test = get_tf_idf(processed_tweets,tokens)\n",
    "print(X_test.shape)\n",
    "Y_pred1 = predict(lr, X_test)\n",
    "print(Y_pred1.shape)\n",
    "Y_pred2 = predict(lr2, X_test)\n",
    "print(Y_pred2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "32878f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.55      0.16      0.24        70\n",
      "           0       0.40      0.32      0.36       126\n",
      "           1       0.86      0.94      0.90       804\n",
      "\n",
      "    accuracy                           0.81      1000\n",
      "   macro avg       0.60      0.47      0.50      1000\n",
      "weighted avg       0.78      0.81      0.78      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "test_y1 = np.array(test_y1)\n",
    "print(test_y1.shape)\n",
    "print(classification_report(test_y1, Y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "29d69977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      advice       0.00      0.00      0.00        10\n",
      "   celebrity       0.86      0.79      0.82       145\n",
      "   info_news       0.69      0.87      0.77       545\n",
      "      others       0.25      0.06      0.10        17\n",
      "    personal       0.52      0.51      0.52       128\n",
      "        plan       0.24      0.09      0.13        82\n",
      "    requests       0.33      0.05      0.09        20\n",
      "restrictions       0.00      0.00      0.00         2\n",
      "      rumors       0.00      0.00      0.00        15\n",
      "   unrelated       0.56      0.25      0.35        36\n",
      "\n",
      "    accuracy                           0.67      1000\n",
      "   macro avg       0.35      0.26      0.28      1000\n",
      "weighted avg       0.62      0.67      0.63      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc1\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pc1\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pc1\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "test_y2 = np.array(test_y2)\n",
    "print(test_y2.shape)\n",
    "print(classification_report(test_y2, Y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "908fc2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n",
      "2000\n",
      "(2000, 12194)\n",
      "(1000,)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "df_test2 = pd.read_csv('test.csv')\n",
    "test2_x =df_test2['text'].tolist()\n",
    "test2_x = np.array(test2_x)\n",
    "print(test2_x.shape)\n",
    "processed_tweets = process_tweets(test2_x)\n",
    "print(len(processed_tweets))\n",
    "X_test2 = get_tf_idf(processed_tweets,tokens)\n",
    "print(X_test2.shape)\n",
    "Y2_pred1 = predict(lr, X_test2)\n",
    "print(Y_pred1.shape)\n",
    "Y2_pred2 = predict(lr2, X_test2)\n",
    "print(Y_pred2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b6cf84bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2['category'] = Y2_pred2\n",
    "df_test2['stance'] = Y2_pred1\n",
    "df_test2.to_csv('sub.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "be0515c033b7cb9134c9f38e8626ac08b58afdd1ae9ecb08f5099615028060d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
